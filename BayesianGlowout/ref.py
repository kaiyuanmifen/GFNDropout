# -*- coding: utf-8 -*-
"""GFN dropout

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xuv7LrVoieo9hDUGSXbn9B7Kri_9xiB7
"""

# Commented out IPython magic to ensure Python compatibility.
import warnings

warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import time
import h5py
from scipy.ndimage.interpolation import rotate

import matplotlib
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import matplotlib.gridspec as gridspec

import seaborn as sns
# %matplotlib inline

import torch
import torchvision
from torchvision import datasets
from torchvision import transforms
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data.sampler import SubsetRandomSampler

import pymc3 as pm

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(torch.__version__)
print(torchvision.__version__)

transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0), (1))])

trainset = datasets.MNIST(root='data/', train=True, download=True, transform=transform)
testset = datasets.MNIST(root='data/', train=False, transform=transform)

indices = torch.randperm(len(trainset))[:3000]

trainset = torch.utils.data.Subset(trainset, indices)

indices = torch.randperm(len(testset))[:300]

testset = torch.utils.data.Subset(testset, indices)

print("training set length")
print(len(trainset))

print("test set length")
print(len(testset))

# Visualize 10 image samples in MNIST dataset
trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)
dataiter = iter(trainloader)
images, labels = dataiter.next()
# plot 10 sample images

_, ax = plt.subplots(1, 10)
ax = ax.flatten()
iml = images[0].numpy().shape[1]
[ax[i].imshow(np.transpose(images[i].numpy(), (1, 2, 0)).reshape(iml, -1), cmap='Greys') for i in range(10)]
[ax[i].set_axis_off() for i in range(10)]
plt.show()
print('label:', labels[:10].numpy())
print('image data shape:', images[0].numpy().shape)


#############dropout model

class DIY_Dropout(nn.Module):
    '''
    DIY version of Dropout, to make sure it is fair to compare with our method, torch offical implementation is computationally efficient but mathematically equivalent
    '''

    def __init__(self, p=0.5):
        super(DIY_Dropout, self).__init__()
        self.p = p
        # multiplier is 1/(1-p). Set multiplier to 0 when p=1 to avoid error...
        if self.p < 1:
            self.multiplier_ = 1.0 / (1.0 - p)
        else:
            self.multiplier_ = 0.0

    def forward(self, input):
        # if model.eval(), don't apply dropout
        if not self.training:
            return input

        # So that we have `input.shape` numbers of Bernoulli(1-p) samples
        selected_ = torch.Tensor(input.shape).uniform_(0, 1) > self.p

        # To support both CPU and GPU.
        if input.is_cuda:
            selected_ = Variable(selected_.type(torch.cuda.FloatTensor), requires_grad=False)
        else:
            selected_ = Variable(selected_.type(torch.FloatTensor), requires_grad=False)

        # Multiply output by multiplier as described in the paper [1]
        return torch.mul(selected_, input) * self.multiplier_


class Mask_Dropout(nn.Module):
    '''
    Mask_Dropout, that takes in a mask to decide which unit to drop
    '''

    def __init__(self):
        super(Mask_Dropout, self).__init__()

    def forward(self, input, mask):
        # if model.eval(), don't apply dropout
        if not self.training:
            return input

        p = 1 - 1.0 * mask.sum() / (mask.shape[0] * mask.shape[1])  # % of unit dropped

        selected_ = mask  # mask is a binary vector with the same shape as input, where 1 indicate not dropping and 0 indicates dropping

        # multiplier is 1/(1-p). Set multiplier to 0 when p=1 to avoid error...
        if p < 1:
            self.multiplier_ = 1.0 / (1.0 - p)
        else:
            self.multiplier_ = 0.0

        # To support both CPU and GPU.
        if input.is_cuda:
            selected_ = Variable(selected_.type(torch.cuda.FloatTensor), requires_grad=False)
        else:
            selected_ = Variable(selected_.type(torch.FloatTensor), requires_grad=False)

        # Multiply output by multiplier as described in the paper [1]

        return torch.mul(selected_, input) * self.multiplier_


##############task model


class MLP(nn.Module):
    def __init__(self, hidden_size=10, droprates=[0, 0]):
        super(MLP, self).__init__()

        self.fc1 = nn.Linear(28 * 28, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, 10)

        self.droprates = droprates

    def forward(self, x):
        x = x.view(x.shape[0], 28 * 28)
        # x=DIY_Dropout(p=self.droprates[0])(x)
        x = self.fc1(x)
        x = DIY_Dropout(p=self.droprates[1])(x)
        x = self.fc2(x)
        # x=DIY_Dropout(p=self.droprates[1])(x)
        x = self.fc3(x)
        return x


class MLP_MaskedDropout(nn.Module):
    def __init__(self, hidden_size=10):
        super(MLP_MaskedDropout, self).__init__()

        self.fc1 = nn.Linear(28 * 28, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, 10)

    def forward(self, x, masks):
        x = x.view(x.shape[0], 28 * 28)
        x = self.fc1(x)
        x = Mask_Dropout()(x, masks[0])
        x = self.fc2(x)
        # x=Mask_Dropout()(x,masks[1])
        x = self.fc3(x)
        return x


##########CNN code
class CNN(nn.Module):
    def __init__(self, droprates=[0, 0]):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)
        self.conv2 = nn.Conv2d(32, 32, kernel_size=5)
        self.conv3 = nn.Conv2d(32, 64, kernel_size=5)
        self.fc1 = nn.Linear(3 * 3 * 64, 64)
        self.fc2 = nn.Linear(64, 10)

        self.droprates = droprates

    def forward(self, x):
        x = F.dropout(x, p=self.droprates[0], training=self.training)
        x = F.relu(self.conv1(x))
        # x = F.dropout(x, p=0.5, training=self.training)
        x = F.relu(F.max_pool2d(self.conv2(x), 2))
        x = F.dropout(x, p=self.droprates[1], training=self.training)
        x = F.relu(F.max_pool2d(self.conv3(x), 2))
        x = F.dropout(x, p=self.droprates[1], training=self.training)
        x = x.view(-1, 3 * 3 * 64)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, p=self.droprates[1], training=self.training)
        x = self.fc2(x)
        return x


class FlowFunction(nn.Module):

    def __init__(self, state_dim, n_action, condition_dim):
        super().__init__()

        self.embedding_dim = 32
        self.hidden_dim = 32

        self.state_dim = state_dim
        self.n_action = n_action

        self.condition_dim = condition_dim

        self.embed = nn.Embedding(self.n_action, self.embedding_dim)

        self.FFL1 = nn.Linear(self.embedding_dim + state_dim + condition_dim, self.hidden_dim)
        self.FFL2 = nn.Linear(self.hidden_dim, self.hidden_dim)
        self.outputlayer = nn.Linear(self.hidden_dim, 1)

    def forward(self, state, action, conditions):
        '''
        state has shape (bsz,state_dim)
        action is a one hot vector has shape (bsz,n_action_space)
        conditions has shape (bsz, condition_dim)
        This function output Log(flow) for numeric issues
        '''

        emebeded_actions = self.embed_code(torch.argmax(action, 1))

        # print("state")
        # print(state.shape)
        # print(emebeded_actions.shape)
        x = torch.cat([state, emebeded_actions, conditions], 1)

        x = nn.LeakyReLU()(self.FFL1(x))
        x = nn.LeakyReLU()(self.FFL2(x))
        output = self.outputlayer(x)

        return output

    def embed_code(self, embed_id):
        return F.embedding(embed_id, self.embed.weight)


class GFN_SamplingMask(object):
    def __init__(self, N_units, device="cpu", batch_size=32, p=0.5, Gamma=0.1):
        '''
        in this version the DAG is not a tree (a state may have mutiple parents)

        a state is a binary vector indicating whether a unit is active (not dropped) or inactive

        an action is to deactivate an unit so that the DAG in acyclic

        '''
        self.N_units = N_units
        self.batch_size = batch_size

        self.action_indice = torch.arange(0, N_units).to(device)  # which unit to deactivate

        self.p = p  # percentage of decativation , to be consistant with traditional dropout method

        self.action_space = torch.nn.functional.one_hot(self.action_indice.flatten()).to(device)

        self.Gamma = Gamma  # chances of randomly choose allowed actions

        self.Train = True  ###switch on/ff random actions selection

        self.device = device

    def reset(self):

        self.action_indice = torch.arange(self.N_units).to(self.device)  # which unit to deactivate

        self.action_space = torch.nn.functional.one_hot(self.action_indice.flatten()).to(self.device)

        self.Train = True

    def UpdateState(self, state, action):
        '''
        state has shape  (dim,)
        action are in the form of one hot vectors
        '''

        action_index = torch.argmax(action)

        state[action_index] = 0  # drop one unit in the dropout

        return state

    def get_flow_in(self, state, FlowFunction, condition):
        '''
        state has shape (dim,)
        get the in flow of a state, in this env all state transition are deterministic

        In this version with a tree structured of DAG, each state only has one parental state

        The order does not matter (a set), there fore the parental states can be any state with the current state + backward action
        '''

        if (state == 0).sum() == 0:
            print("initial state ,no parents")

        else:
            backward_action_indice = []
            parent_states = []

            for Position_index in range(state.shape[0]):
                if Position_index == 0:
                    parent_state = state.detach().clone()
                    parent_state[Position_index] = 1

                backward_action_index = Position_index

                parent_states.append(parent_state.unsqueeze(0))

                backward_action_indice.append(backward_action_index)

            parent_states = torch.cat(parent_states, 0)
            backward_action_indice = torch.tensor(backward_action_indice)

        backward_actions = self.action_space.clone().detach()[backward_action_indice, :]

        conditions = condition.unsqueeze(0).repeat(parent_states.shape[0], 1)

        LogFlows = FlowFunction(parent_states, backward_actions, conditions)

        return parent_states, backward_actions, LogFlows

    def get_flow_out(self, state, FlowFunction, condition):

        PossibleActions = self.action_space[state == 1, :]

        state_vec = state.unsqueeze(0).repeat((state == 1).sum(), 1)

        condition = condition.unsqueeze(0).repeat((state == 1).sum(), 1)

        LogFlows = FlowFunction(state_vec, PossibleActions, condition)

        return state, PossibleActions, LogFlows

    def forward(self, FlowFunction, conditions):
        '''
        the flow function conditions on inputs into the task model , the condition has shape (batch_sz, dim)
        '''

        ###initialize state
        self.states = torch.ones((conditions.shape[0], self.N_units)).to(
            self.device)  # all ones,intial states of the GFN where all units are active

        self.trajectories = []  # trjactories may have differnet length

        # run the forward

        terminal = False  # if the trajectories has come to an end

        for step in range(0, int(self.N_units * self.p)):

            terminal = len(self.trajectories) == int(self.N_units * self.p)  # reserved for later improvement

            if not terminal:
                states_vec = []
                for i in range(conditions.shape[0]):  # in the last batch, N_samples may be < batch size
                    state = self.states.clone().detach()[i, :]

                    _, PossibleActions, LogFlows = self.get_flow_out(state, FlowFunction, conditions[i, :])

                    Flows = torch.exp(LogFlows)

                    action_Prob = torch.nn.Softmax(0)(Flows)

                    if np.random.uniform() < self.Gamma and self.Train == True:  # with some chances of random actions
                        # print("using random actions")
                        action_Prob = torch.zeros_like(action_Prob).fill_(1.0 / action_Prob.shape[0])

                    action_index_chosen = np.random.choice(np.arange(0, PossibleActions.shape[0]),
                                                           p=action_Prob.flatten().detach().cpu().numpy())

                    action = PossibleActions[action_index_chosen, :]

                    state = self.UpdateState(state, action)

                    states_vec.append(state.unsqueeze(0))

                self.states = torch.cat(states_vec, 0)

            self.trajectories.append(self.states.detach().clone())

        return self.states

    def CalculateFlowMatchingLoss(self, FlowFunction, reward_terminal, conditions):
        '''
        first run the step_forward function to generate batch of trajectgories ending with terminal states
        then run this function to calculate flow matching loss
        the reward_terminal is calcualted from terminate states(self.states afte running forward function), it has the shape (batch_size,)

        '''

        ####low match on all but initial state
        Epsilon = 1e-8  # small number to avoid numeric problem to avoid log on tiny number, and balance large and small flow, larger number smooth out the distribution (more difficult for peaky modes)

        Match_loss_all = 0
        for t in range(1, len(self.trajectories)):
            states_t = self.trajectories[t]  # shape Batch_size X N_slots Xstate_dim

            for i in range(conditions.shape[0]):  # loop through the batch dimension

                state = states_t[i, :]

                terminal = t == int(self.N_units * self.p)

                if not terminal:
                    _, _, LogFlowsOut = self.get_flow_out(state, FlowFunction, conditions[i, :])

                    Reward = torch.tensor(0.0)
                else:
                    LogFlowsOut = torch.tensor(0.0)
                    Reward = torch.tensor(reward_terminal[i]).float()
                _, _, LogFlowsIn = self.get_flow_in(state, FlowFunction, conditions[i, :])

                ####hand-written version:
                # In_log_sum=torch.log(Epsilon+torch.exp(LogFlowsIn).sum())
                # Out_log_sum=torch.log(Epsilon+Reward+torch.exp(LogFlowsOut).sum())

                ###in torch logsumexp version is more numerically stable:

                In_log_sum = torch.logsumexp(torch.cat([torch.log(torch.tensor(Epsilon).reshape(1, 1)).to(self.device),
                                                        LogFlowsIn], 0), 0)
                Out_log_sum = torch.logsumexp(torch.cat([torch.log(torch.tensor(Epsilon).reshape(1, 1)).to(self.device),
                                                         torch.log(torch.tensor(Reward).reshape(1, 1).to(self.device)),
                                                         LogFlowsOut], 0), 0)

                Match_loss_ti = (In_log_sum - Out_log_sum) ** 2

                Match_loss_all += Match_loss_ti

        return Match_loss_all


class MLPClassifier:
    def __init__(self, droprates=[0, 0], batch_size=128, max_epoch=10, \
                 lr=0.001, momentum=0, model_type="MLP", N_units=20):
        # Wrap MLP model
        self.droprates = droprates
        self.batch_size = batch_size
        self.max_epoch = max_epoch
        self.model_type = model_type

        if self.model_type == "MLP":
            self.model = MLP(droprates=droprates)
        elif self.model_type == "CNN":
            self.model = CNN(droprates)
        elif self.model_type == "MLP_GFN":
            self.model = MLP_MaskedDropout(N_units)

        total_params = sum(p.numel() for p in self.model.parameters())
        print("number of parameters:")
        print(total_params)

        self.model.to(device)
        self.criterion = nn.CrossEntropyLoss().to(device)
        self.optimizer = optim.SGD(self.model.parameters(), lr=lr, momentum=momentum)

        self.loss_ = []
        self.test_accuracy = []
        self.test_error = []

        #####GFN flow function and samplling function

        self.GFN_operation = GFN_SamplingMask(N_units=N_units, batch_size=batch_size, device=device, p=0.5, Gamma=0.1)

        self.Fnet = FlowFunction(state_dim=N_units, n_action=N_units, condition_dim=28 * 28).to(device)

        self.optimizer_GFN = optim.SGD(self.Fnet.parameters(), lr=lr, momentum=momentum)

    def fit(self, trainset, testset, verbose=True):
        # Training, make sure it's on GPU, otherwise, very slow...
        trainloader = torch.utils.data.DataLoader(trainset, batch_size=self.batch_size, shuffle=True)
        testloader = torch.utils.data.DataLoader(testset, batch_size=len(testset), shuffle=False)
        X_test, y_test = iter(testloader).next()
        X_test = X_test.to(device)
        for epoch in range(self.max_epoch):

            running_loss = 0
            for i, data in enumerate(trainloader, 0):

                inputs, labels = data
                inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)
                self.optimizer.zero_grad()

                if self.model_type == "MLP_GFN":

                    self.GFN_operation.reset()
                    selected_ = self.GFN_operation.forward(FlowFunction=self.Fnet,
                                                           conditions=inputs.reshape(inputs.shape[0], 28 * 28))

                    # selected_ = torch.zeros((inputs.shape[0],10)).uniform_(0,1)>0.5
                    masks = [selected_, selected_]
                    outputs = self.model(inputs, masks)  #####if using GFN to generate mask for dropout

                else:
                    outputs = self.model(inputs)

                loss = self.criterion(outputs, labels)
                loss.backward()
                self.optimizer.step()
                running_loss += loss.item()

                #####matching loss to update GFN
                if self.model_type == "MLP_GFN":
                    self.optimizer_GFN.zero_grad()
                    loss_batch = nn.CrossEntropyLoss(reduce=False)(outputs,
                                                                   labels)  # rewards required for each element in the batch
                    beta = 0.1  ###higher parameter to tune sharpness of the peak
                    rewards = torch.exp(-beta * loss_batch).detach().clone()

                    Match_loss = self.GFN_operation.CalculateFlowMatchingLoss(self.Fnet, rewards,
                                                                              conditions=inputs.reshape(inputs.shape[0],
                                                                                                        28 * 28).detach().clone())
                    Match_loss.backward()
                    self.optimizer_GFN.step()

            self.loss_.append(running_loss / len(trainloader))
            if verbose and epoch % 5 == 0:
                print('Epoch {} loss: {}'.format(epoch + 1, self.loss_[-1]))
            y_test_pred = self.predict(X_test).cpu()

            self.test_accuracy.append(np.mean((y_test == y_test_pred).numpy()))
            self.test_error.append(int(len(testset) * (1 - self.test_accuracy[-1])))
            if verbose and epoch % 5 == 0:
                print('Test error: {}; test accuracy: {}'.format(self.test_error[-1], self.test_accuracy[-1]))
        return self

    def predict(self, x):
        # Used to keep all test errors after each epoch
        model = self.model.eval()
        with torch.no_grad():
            if self.model_type == "MLP_GFN":
                self.GFN_operation.reset()
                self.GFN_operation.Train = False
                selected_ = self.GFN_operation.forward(FlowFunction=self.Fnet,
                                                       conditions=x.reshape(x.shape[0], 28 * 28))

                # selected_ = torch.zeros((inputs.shape[0],10)).uniform_(0,1)>0.5
                masks = [selected_, selected_]
                outputs = self.model(Variable(x), masks)  #####if using GFN to generate mask for dropout
            else:
                outputs = model(Variable(x))

            _, pred = torch.max(outputs.data, 1)
        model = self.model.train()
        return pred

    def __str__(self):
        return 'Hidden layers: {}; dropout rates: {}'.format(self.hidden_layers, self.droprates)


### Below is training code, uncomment to train your own model... ###
### Note: You need GPU to run this section ###
N_units = 30
# Define networks
mlp1 = [MLPClassifier(droprates=[0.0, 0.5], max_epoch=50, model_type="MLP", N_units=N_units),
        MLPClassifier(droprates=[0.0, 0.0], max_epoch=50, model_type="MLP_GFN", N_units=N_units)]
#
# Training, set verbose=True to see loss after each epoch.
[mlp.fit(trainset, testset, verbose=True) for mlp in mlp1]

# Save torch models
for ind, mlp in enumerate(mlp1):
    torch.save(mlp.model, 'mnist_mlp1_' + str(ind) + '.pth')
    # Prepare to save errors
    mlp.test_error = list(map(str, mlp.test_error))

# Save test errors to plot figures
open("mlp1_test_errors.txt", "w").write('\n'.join([','.join(mlp.test_error) for mlp in mlp1]))

# Load saved models to CPU
# mlp1_models = [torch.load('mnist_mlp1_'+str(ind)+'.pth',map_location={'cuda:0': 'cpu'}) for ind in [0,1,2]]

# Load saved test errors to plot figures.
mlp1_test_errors = [error_array.split(',') for error_array in open("mlp1_test_errors.txt", "r").read().split('\n')]
mlp1_test_errors = np.array(mlp1_test_errors, dtype='f')

print(len(mlp1_test_errors))

labels = ['original', 'GFN dropout']
#          'MLP 50% dropout in hidden layers',
#         'MLP 50% dropout in hidden layers+20% input layer']

plt.figure(figsize=(8, 7))
for i, r in enumerate(mlp1_test_errors):
    plt.plot(range(1, len(r) + 1), r, '.-', label=labels[i], alpha=0.6)
# plt.ylim([50, 250])
plt.legend(loc=1)
plt.xlabel('Epochs')
plt.ylabel('Number of errors in test set')
plt.title('Test error on MNIST dataset for Multilayer Perceptron N_units=' + str(N_units))
plt.show()
